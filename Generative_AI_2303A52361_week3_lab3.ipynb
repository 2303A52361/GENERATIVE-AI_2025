{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCkqwd6GES1dgE+3ghg4C6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303A52361/GENERATIVE-AI_2025/blob/main/Generative_AI_2303A52361_week3_lab3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. (1 ponto) Write Python code without using any libraries to find the value of x at which the\n",
        "function f(x) shown in equation (1) has minimum value. Consider Gradient Descent Algorithm.\n",
        "f (x) = 5x4 + 3x2 + 10"
      ],
      "metadata": {
        "id": "fXjOSWl7u95S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRjApNietaEj",
        "outputId": "e9b16cc3-7b02-4fe5-c053-1c49edd6c6f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The value of x that minimizes f(x) is: 0.001161634189870079\n"
          ]
        }
      ],
      "source": [
        "def f(x):\n",
        "    return 5 * x*4 + 3 * x*2 + 10\n",
        "def df(x):\n",
        "    return 20 * x**3 + 6 * x\n",
        "def gradient_descent_f(learning_rate=0.001, iterations=1000):\n",
        "    x = 1\n",
        "    for i in range(iterations):\n",
        "        grad = df(x)\n",
        "        x = x - learning_rate * grad\n",
        "    return x\n",
        "x_min_f = gradient_descent_f()\n",
        "print(f\"The value of x that minimizes f(x) is: {x_min_f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. (1 ponto) Write Python code without using any libraries to find the value of x and y at which the\n",
        "function g(x,y) shown in equation (2) has minimum value. Consider Gradient Descent Algorithm.\n",
        "f (x) = 3x2 + 5e−y + 10"
      ],
      "metadata": {
        "id": "JOMuFdH0vnIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "def g(x, y):\n",
        "    return 3 * x**2 + 5 * math.exp(-y) + 10\n",
        "def dg_dx(x, y):\n",
        "    return 6 * x\n",
        "def dg_dy(x, y):\n",
        "    return -5 * math.exp(-y)\n",
        "def gradient_descent_g(learning_rate=0.01, iterations=1000):\n",
        "    x, y = 10, 10\n",
        "    for i in range(iterations):\n",
        "        grad_x = dg_dx(x, y)\n",
        "        grad_y = dg_dy(x, y)\n",
        "        x = x - learning_rate * grad_x\n",
        "        y = y - learning_rate * grad_y\n",
        "    return x, y\n",
        "x_min_g, y_min_g = gradient_descent_g()\n",
        "print(f\"The values of x and y that minimize g(x, y) are: x = {x_min_g}, y = {y_min_g}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVPruW8NvvS6",
        "outputId": "3ce38c08-f9e3-4fd9-ddd4-17af71d9091a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The values of x and y that minimize g(x, y) are: x = 1.3423123924933693e-26, y = 10.002267426506178\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. (1 ponto) Write Python code without using any libraries to find the value of x at which the\n",
        "sigmoid function z(x) shown in equation (3) has minimum value. Consider Gradient Descent\n",
        "Algorithm.\n",
        "z(x) = 1\n",
        "1 + e−x (3)"
      ],
      "metadata": {
        "id": "4kbzHt_xwrTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def z(x):\n",
        "    return 1 / (1 + math.exp(-x))\n",
        "def dz_dx(x):\n",
        "    sigmoid = 1 / (1 + math.exp(-x))\n",
        "    return sigmoid * (1 - sigmoid)\n",
        "\n",
        "def gradient_descent_z(learning_rate=0.01, iterations=1000):\n",
        "    x = 10\n",
        "    for i in range(iterations):\n",
        "        grad = dz_dx(x)\n",
        "        x = x - learning_rate * grad\n",
        "    return x\n",
        "\n",
        "\n",
        "x_min_z = gradient_descent_z()\n",
        "print(f\"The value of x that minimizes z(x) is: {x_min_z}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTJTEY44w9RK",
        "outputId": "b87b6093-b2be-4b66-e51f-7121ac4d5273"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The value of x that minimizes z(x) is: 9.9995459389649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. (1 ponto) Write Python code without using any libraries to find the value of optimal values of\n",
        "model parameters M and C such that the model’s Square Error Value shown in equation 4 will\n",
        "be minimum. It means model gives output close to expected output as shown in Figure 1\n"
      ],
      "metadata": {
        "id": "uosYgU81xKEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X= [1, 2, 3, 4, 5]\n",
        "Y = [2, 4, 6, 8, 10]\n",
        "\n",
        "def predicted_output(x, M, C):\n",
        "    return M * x + C\n",
        "\n",
        "def square_error(M, C):\n",
        "    total_error = 0\n",
        "    for x, y in zip(X, Y):\n",
        "        y_pred = predicted_output(x, M, C)\n",
        "        total_error += (y - y_pred)**2\n",
        "    return total_error\n",
        "\n",
        "\n",
        "def grad_se_M(M, C):\n",
        "    grad = 0\n",
        "    for x, y in zip(X, Y):\n",
        "        grad += -2 * x * (y - (M * x + C))\n",
        "    return grad\n",
        "\n",
        "def grad_se_C(M, C):\n",
        "    grad = 0\n",
        "    for x, y in zip(X, Y):\n",
        "        grad += -2 * (y - (M * x + C))\n",
        "    return grad\n",
        "\n",
        "\n",
        "def gradient_descent_se(learning_rate=0.01, iterations=1000):\n",
        "    M, C = 0, 0\n",
        "    for i in range(iterations):\n",
        "        grad_M = grad_se_M(M, C)\n",
        "        grad_C = grad_se_C(M, C)\n",
        "        M = M - learning_rate * grad_M\n",
        "        C = C - learning_rate * grad_C\n",
        "    return M, C\n",
        "M_optimal, C_optimal = gradient_descent_se()\n",
        "print(f\"The optimal values of M and C that minimize the square error are: M = {M_optimal}, C = {C_optimal}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cI14Mi8xYEu",
        "outputId": "96390181-c156-4cae-abc6-3c4892a10c42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The optimal values of M and C that minimize the square error are: M = 1.9999999943842544, C = 2.0274623625998433e-08\n"
          ]
        }
      ]
    }
  ]
}